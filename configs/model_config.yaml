model:
  cnn:
    activation: 'ReLU'
    normalization: 'LayerNorm'
    polling: 'MaxPool1d'
    out_features: [100, 50, 20]
    pooling_sizes: [2]
    kernel_sizes: [1]
    strides: [2]
    dilations: [1]
    dropout_rates: [0.4]
    fc_outputs: [50, 1]
    fc_dropout_rates: [0.4, 0.4]
  lstm:
    out_features: [50, 20]
    normalization: 'LayerNorm'
    bidirectional: True
    activation: 'ReLU'
    dropout_rates: [0.4, 0.4]
    fc_outputs: [40, 1]
    fc_dropout_rates: [0.4, 0.4]
  tsmixer:
    out_seq_len: 1
    n_block: 10
    # target_slice: [1]
    normalization: 'LayerNorm'
    dropout_rates: 0.4
    ff_dim: 10
    activation: 'ReLU'
  grunet:
    block:
        cell_type: GRU
        activation: 'LeakyReLU'
        normalization_layer: LayerNorm
        hidden_size: 256
        n_stacks: 1
        n_layers: 2
        dropout_rates: 0.4
        bidirectional: True
    rnn:
      hidden_size: 256
      output_size: 64
      dropout_rates: 0.4
      n_layers: 5
      cell_type: GRU
      activation: 'LeakyReLU'
      normalization_layer: LayerNorm
    encoder_block:
      activation: 'LeakyReLU'
      normalization_layer: LayerNorm
      dropout_rates: 0.4
    grunet:
      out_channels: [8, 32, 64]
      kernel_sizes: [17, 11, 7]
      strides: [2, 2, 2]
      dilations: [1]
      dropout_rates: 0.4
      deconvolution_paddings: [5]

